from __future__ import annotations

from typing import Any, Optional

from ..._typing import DataLayoutND, Tensor
from ...tensor.math import tanh as tanh
from ...tensor.math import tanh_ as tanh_

def celu(x: Tensor, alpha: float = ..., name: str | None = ...) -> Tensor: ...
def elu(x: Tensor, alpha: float = ..., name: str | None = ...) -> Tensor: ...
def elu_(x: Tensor, alpha: float = ..., name: str | None = ...) -> Tensor: ...
def gelu(x: Tensor, approximate: bool = ..., name: str | None = ...) -> Tensor: ...
def hardshrink(x: Tensor, threshold: float = ..., name: str | None = ...) -> Tensor: ...
def hardtanh(x: Tensor, min: float = ..., max: float = ..., name: str | None = ...) -> Tensor: ...
def hardsigmoid(x: Tensor, slope: float = ..., offset: float = ..., name: str | None = ...) -> Tensor: ...
def hardswish(x: Tensor, name: str | None = ...) -> Tensor: ...
def leaky_relu(x: Tensor, negative_slope: float = ..., name: str | None = ...) -> Tensor: ...
def prelu(
    x: Tensor,
    weight: Tensor,
    data_format: DataLayoutND = ...,
    name: str | None = ...,
) -> Tensor: ...
def relu(x: Tensor, name: str | None = ...) -> Tensor: ...
def relu_(x: Tensor, name: str | None = ...) -> Tensor: ...
def log_sigmoid(x: Tensor, name: str | None = ...) -> Tensor: ...
def maxout(x: Tensor, groups: Any, axis: int = ..., name: str | None = ...) -> Tensor: ...
def relu6(x: Tensor, name: str | None = ...) -> Tensor: ...
def selu(x: Tensor, scale: float = ..., alpha: float = ..., name: str | None = ...) -> Tensor: ...
def silu(x: Tensor, name: str | None = ...) -> Tensor: ...
def softmax(x: Tensor, axis: int = ..., dtype: Any | None = ..., name: str | None = ...) -> Tensor: ...
def softmax_(x: Tensor, axis: int = ..., dtype: Any | None = ..., name: str | None = ...) -> Tensor: ...
def softplus(x: Tensor, beta: int = ..., threshold: int = ..., name: str | None = ...) -> Tensor: ...
def softshrink(x: Tensor, threshold: float = ..., name: str | None = ...) -> Tensor: ...
def softsign(x: Tensor, name: str | None = ...) -> Tensor: ...
def swish(x: Tensor, name: str | None = ...) -> Tensor: ...
def mish(x: Tensor, name: str | None = ...) -> Tensor: ...
def tanhshrink(x: Tensor, name: str | None = ...) -> Tensor: ...
def thresholded_relu(x: Tensor, threshold: float = ..., name: str | None = ...) -> Tensor: ...
def log_softmax(x: Any, axis: int = ..., dtype: Any | None = ..., name: str | None = ...) -> Tensor: ...
def glu(x: Tensor, axis: int = ..., name: str | None = ...) -> Tensor: ...
def gumbel_softmax(
    x: Tensor, temperature: float = ..., hard: bool = ..., axis: int = ..., name: str | None = ...
) -> Tensor: ...
def sigmoid(x: Tensor, name: str | None = ...) -> Tensor: ...
