from __future__ import annotations

from typing import Any, Optional

from paddle.nn import Layer
from typing_extensions import Literal

from ..._typing import Tensor
from ...framework import ParamAttr

class CELU(Layer):
    def __init__(self, alpha: float = ..., name: Optional[str] = ...) -> None: ...
    def forward(self, x: Tensor) -> Tensor: ...
    def extra_repr(self) -> str: ...

class ELU(Layer):
    def __init__(self, alpha: float = ..., name: Optional[str] = ...) -> None: ...
    def forward(self, x: Tensor) -> Tensor: ...
    def extra_repr(self) -> str: ...

class GELU(Layer):
    def __init__(self, approximate: bool = ..., name: Optional[str] = ...) -> None: ...
    def forward(self, x: Tensor) -> Tensor: ...
    def extra_repr(self) -> str: ...

class Hardshrink(Layer):
    def __init__(self, threshold: float = ..., name: Optional[str] = ...) -> None: ...
    def forward(self, x: Tensor) -> Tensor: ...
    def extra_repr(self) -> str: ...

class Hardswish(Layer):
    def __init__(self, name: Optional[str] = ...) -> None: ...
    def forward(self, x: Tensor) -> Tensor: ...
    def extra_repr(self) -> str: ...

class Tanh(Layer):
    def __init__(self, name: Optional[str] = ...) -> None: ...
    def forward(self, x: Tensor) -> Tensor: ...
    def extra_repr(self) -> str: ...

class Hardtanh(Layer):
    def __init__(self, min: float = ..., max: float = ..., name: Optional[str] = ...) -> None: ...
    def forward(self, x: Tensor) -> Tensor: ...
    def extra_repr(self) -> str: ...

class PReLU(Layer):
    def __init__(
        self,
        num_parameters: int = ...,
        init: float = ...,
        weight_attr: Optional[ParamAttr] = ...,
        data_format: Literal["NC", "NCL", "NCHW", "NCDHW", "NLC", "NHWC", "NDHWC"] = ...,
        name: Optional[str] = ...,
    ) -> None: ...
    def forward(self, x: Tensor) -> Tensor: ...
    def extra_repr(self) -> str: ...

class ReLU(Layer):
    def __init__(self, name: Optional[str] = ...) -> None: ...
    def forward(self, x: Tensor) -> Tensor: ...
    def extra_repr(self) -> str: ...

class ReLU6(Layer):
    def __init__(self, name: Optional[str] = ...) -> None: ...
    def forward(self, x: Tensor) -> Tensor: ...
    def extra_repr(self) -> str: ...

class SELU(Layer):
    def __init__(self, scale: float = ..., alpha: float = ..., name: Optional[str] = ...) -> None: ...
    def forward(self, x: Tensor) -> Tensor: ...
    def extra_repr(self) -> str: ...

class LeakyReLU(Layer):
    def __init__(self, negative_slope: float = ..., name: Optional[str] = ...) -> None: ...
    def forward(self, x: Tensor) -> Tensor: ...
    def extra_repr(self) -> str: ...

class Sigmoid(Layer):
    def __init__(self, name: Optional[str] = ...) -> None: ...
    def forward(self, x: Tensor) -> Tensor: ...
    def extra_repr(self) -> str: ...

class Hardsigmoid(Layer):
    def __init__(self, name: Optional[str] = ...) -> None: ...
    def forward(self, x: Tensor) -> Tensor: ...
    def extra_repr(self) -> str: ...

class Softplus(Layer):
    def __init__(self, beta: int = ..., threshold: int = ..., name: Optional[str] = ...) -> None: ...
    def forward(self, x: Tensor) -> Tensor: ...
    def extra_repr(self) -> str: ...

class Softshrink(Layer):
    def __init__(self, threshold: float = ..., name: Optional[str] = ...) -> None: ...
    def forward(self, x: Tensor) -> Tensor: ...
    def extra_repr(self) -> str: ...

class Softsign(Layer):
    def __init__(self, name: Optional[str] = ...) -> None: ...
    def forward(self, x: Tensor) -> Tensor: ...
    def extra_repr(self) -> str: ...

class Swish(Layer):
    def __init__(self, name: Optional[str] = ...) -> None: ...
    def forward(self, x: Tensor) -> Tensor: ...
    def extra_repr(self) -> str: ...

class Mish(Layer):
    def __init__(self, name: Optional[str] = ...) -> None: ...
    def forward(self, x: Tensor) -> Tensor: ...
    def extra_repr(self) -> str: ...

class Tanhshrink(Layer):
    def __init__(self, name: Optional[str] = ...) -> None: ...
    def forward(self, x: Tensor) -> Tensor: ...
    def extra_repr(self) -> str: ...

class ThresholdedReLU(Layer):
    def __init__(self, threshold: float = ..., name: Optional[str] = ...) -> None: ...
    def forward(self, x: Tensor) -> Tensor: ...
    def extra_repr(self) -> str: ...

class Silu(Layer):
    def __init__(self, name: Optional[str] = ...) -> None: ...
    def forward(self, x: Tensor) -> Tensor: ...
    def extra_repr(self) -> str: ...

class LogSigmoid(Layer):
    def __init__(self, name: Optional[str] = ...) -> None: ...
    def forward(self, x: Tensor) -> Tensor: ...
    def extra_repr(self) -> str: ...

class Softmax(Layer):
    def __init__(self, axis: int = ..., name: Optional[str] = ...) -> None: ...
    def forward(self, x: Tensor) -> Tensor: ...
    def extra_repr(self) -> str: ...

class LogSoftmax(Layer):
    def __init__(self, axis: int = ..., name: Optional[str] = ...) -> None: ...
    def forward(self, x: Tensor) -> Tensor: ...
    def extra_repr(self) -> str: ...

class Maxout(Layer):
    def __init__(self, groups: Any, axis: int = ..., name: Optional[str] = ...) -> None: ...
    def forward(self, x: Tensor) -> Tensor: ...
    def extra_repr(self) -> str: ...
