from __future__ import annotations

from typing import Any, Optional

from paddle.fluid import dygraph_utils as dygraph_utils

def fused_feedforward(
    x: Any,
    linear1_weight: Any,
    linear2_weight: Any,
    linear1_bias: Any | None = ...,
    linear2_bias: Any | None = ...,
    ln1_scale: Any | None = ...,
    ln1_bias: Any | None = ...,
    ln2_scale: Any | None = ...,
    ln2_bias: Any | None = ...,
    dropout1_rate: float = ...,
    dropout2_rate: float = ...,
    activation: str = ...,
    ln1_epsilon: float = ...,
    ln2_epsilon: float = ...,
    pre_layer_norm: bool = ...,
    training: bool = ...,
    mode: str = ...,
    ring_id: int = ...,
    add_residual: bool = ...,
    name: str | None = ...,
): ...
def fused_multi_head_attention(
    x: Any,
    qkv_weight: Any,
    linear_weight: Any,
    pre_layer_norm: bool = ...,
    pre_ln_scale: Any | None = ...,
    pre_ln_bias: Any | None = ...,
    ln_scale: Any | None = ...,
    ln_bias: Any | None = ...,
    pre_ln_epsilon: float = ...,
    qkv_bias: Any | None = ...,
    linear_bias: Any | None = ...,
    cache_kv: Any | None = ...,
    attn_mask: Any | None = ...,
    dropout_rate: float = ...,
    attn_dropout_rate: float = ...,
    ln_epsilon: float = ...,
    training: bool = ...,
    mode: str = ...,
    ring_id: int = ...,
    add_residual: bool = ...,
    name: str | None = ...,
): ...
def fused_multi_transformer(
    x: Any,
    ln_scales: Any,
    ln_biases: Any,
    qkv_weights: Any,
    qkv_biases: Any,
    linear_weights: Any,
    linear_biases: Any,
    ffn_ln_scales: Any,
    ffn_ln_biases: Any,
    ffn1_weights: Any,
    ffn1_biases: Any,
    ffn2_weights: Any,
    ffn2_biases: Any,
    pre_layer_norm: bool = ...,
    epsilon: float = ...,
    cache_kvs: Any | None = ...,
    time_step: Any | None = ...,
    attn_mask: Any | None = ...,
    dropout_rate: float = ...,
    activation: str = ...,
    training: bool = ...,
    mode: str = ...,
    ring_id: int = ...,
    name: str | None = ...,
): ...
