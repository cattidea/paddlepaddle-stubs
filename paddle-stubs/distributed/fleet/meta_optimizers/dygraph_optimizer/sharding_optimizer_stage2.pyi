from __future__ import annotations

from functools import reduce as reduce
from itertools import chain as chain
from typing import Any, Optional

from paddle.optimizer import Optimizer

from ...meta_parallel.sharding.sharding_utils import (
    ShardingClipGrad as ShardingClipGrad,
)
from ...meta_parallel.sharding.sharding_utils import Type as Type
from ...meta_parallel.sharding.sharding_utils import device_guard as device_guard
from ...utils.internal_storage import GradStorage as GradStorage
from ...utils.internal_storage import ParamStorage as ParamStorage

alignment: Any
align: Any

class ShardingOptimizerStage2(Optimizer):
    group: Any = ...
    world_size: Any = ...
    rank: Any = ...
    param_storages: Any = ...
    offload: Any = ...
    offload_device: str = ...
    offload_buffer_size: int = ...
    offload_param2align: Any = ...
    offload_params: Any = ...
    offload_grads: Any = ...
    def __init__(
        self,
        params: Any,
        optim: Any,
        group: Any | None = ...,
        offload: bool = ...,
        device: str = ...,
        pertrain_sync_models: bool = ...,
        **kw: Any,
    ): ...
    @property
    def local_params(self): ...
    @property
    def param2rank(self): ...
    @property
    def dtype_rank_params(self): ...
    @property
    def rank_buffer_size(self): ...
    def step(self) -> None: ...
    def minimize(self) -> None: ...
    def set_state_dict(self, state_dict: Any) -> None: ...
    def state_dict(self): ...
